第5章 分词和标注词汇
5.1 使用词性标注器
5.2 标注语料库
    表示已标注的标识符
    读取已标注的语料库
    简化的词性标记集
    名词
    动词
    形容词和副词
    未简化的标记
    探索已标注的语料库
5.3 使用Python字典映射词及其属性
5.4 自动标注
    默认标注器
    正则表达式标注器
    查询标注器
    评估
5.5 N-gram标注
    一元标注（Unigram Tagging)
    分离训练和测试数据
    一般的N-gram的标注
    组合标注器
    标注生词
    存储标注器
    性能限制
    跨句子边界标注
5.6 基于转换的标注
5.7 如何确定一个词的分类
    形态学线索
    句法线索
    语义线索
    新词
    词性标注集中的形态学
5.8 小结


    名词、动词、形容词和副词之间的差异。

    这些"词类"不是闲置的文法家的发明，而是对许多语言处理任务都有用的分类。

    这些分类源于对文本中词的分布的简单的分析。

    本章的目的是要回答下列问题：
        1. 什么是词汇分类，在自然语言处理中它们是如何使用？
        2. 一个好的存储词汇和它们的分类的Python 数据结构是什么？
        3. 我们如何自动标注文本中词汇的词类？

    我们将介绍NLP 的一些基本技术，包括
        序列标注、N-gram 模型、回退和评估。
    这些技术在许多方面都很有用，标注为我们提供了一个表示它们的简单的上下文。
    我们还将看到标注为何是典型的NLP 流水线中继分词之后的第二个步骤。

    将词汇按它们的词性（parts-of-speech，POS）分类以及相应的标注它们的过程被称为词性标注（part-of-speech tagging, POS tagging）或干脆简称标注。
    词性也称为词类或词汇范畴。
    用于特定任务的标记的集合被称为一个标记集。我们在本章的重点是利用标记和自动标注文本。

5.1 使用词性标注器
    一个词性标注器Part-of-speech tagger或POS tagger处理一个词序列，为每个词附加一个词性标记。

        >>> import nltk
        >>> text = nltk.word_tokenize('And now for someting completely different')
        >>> nltk.pos_tag(text)
        [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('someting', 'VBG'), ('completely', 'RB'), ('different', 'JJ')]

        在这里我们看到and 是CC，并列连词；
                    now 和completely 是RB，副词；
                    for 是IN，介词；
                    something 是NN，名词；
                    different 是JJ，形容词。
    NLTK中提供了每个标记的文档，可以使用标记来查询，如：nltk.help.upenn_tagset('RB')，或正则表达式，如：nltk.help.upenn_brown_tagset('NN.*').

    一个标注器也可以为我们对未知词的认识过程建模；
    例如：我们可以根据词根scrobble猜测scrobbling 可能是一个动词，并有可能发生在he was scrobbling 这样的上下文中。

5.2 标注语料库
5.2.1 表示已标注的标识符
    按照NLTK的约定，一个已标注的标识符使用一个由标识符和标记组成的元组来表示。
    我们可以使用函数str2tuple()从表示一个已标注的标识符的标准字符串创建一个这样的特殊元组:
        >>> tagged_token = nltk.tag.str2tuple('fly/NN')
        >>> tagged_token
        ('fly', 'NN')
        >>> tagged_token[0]
        'fly'
        >>> tagged_token[1]
        'NN'

5.2.2 读取已标注的语料库
    NLTK中包括的若干语料库 已标注 了词性。
    下面是一个你用文本编辑器打开一个布朗语料库的文件就能看到的例子：
        The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr ...

    其他语料库使用各种格式存储词性标记。
    NLTK中的语料库阅读器提供了一个统一的接口，使你不必理会这些不同的文件格式。

    与刚才提取并显示的上面的文件不同，布朗语料库的月料库阅读器按如下所示的方式表示数据。
    注意：部分词性标记已转换为大写的；自从布朗语料库发布以来，这已成为标准的做法。

        >>> nltk.corpus.brown.tagged_words()
        [('The', 'AT'), ('Fulton', 'NP-TL'), ...]

    只要语料库包含已标注的文本，NLTK的语料库接口都将有一个tagged_words()方法。

    并非所有的语料库都采用同一组标记。
    最初，我们想避免这些标记集的复杂化，所以我们使用一个内置的到一个简化的标记集的映射：
        >>> nltk.corpus.brown.tagged_words(tagset='universal')



5.2.3 简化的词性标记集
    已标注的语料库使用许多不同的标记集约定来标注词汇。

        >>> from nltk.corpus import brown
        >>> brown_news_tagged = brown.tagged_words(categories='news',tagset='universal')
        >>> tag_fdist = nltk.FreqDist(tag for (word,tag) in brown_news_tagged)
        >>> tag_fdist.keys()
        >>> tag_fdist.plot(cumulative=True)

    我们可以使用这些标记做强大的搜索，结合一个图形化的POS一致性工具nltk.app.concordance()。
    用它来寻找任一词和POS标记的组合，如：N N N N , hid/VD, hit/VN或the ADJ man.



5.2.4 名词
5.2.5 动词
5.2.6 形容词和副词
5.2.7 未简化的标记
5.2.8 探索已标注的语料库

5.3 使用Python字典映射词及其属性
    (word,tag)形式的一个已标注是 词和词性标记的关联。

    一旦我们开始做词性标注，我们将会创建分配一个标记给一个词的程序，标记是在给定上下文中最可能的标记。
    我们可以认为这个过程是从词到标记的映射。

    在Python中最自然的方式存储映射是使用所谓的字典数据类型（在其他编程语言中又称为 关联数组 或 哈希数组）。

5.4 自动标注
    以不同的方式来给文本自动添加词性标记。

    我们将看到一个词的标记依赖于这个词和它在句子中的上下文。
    出于这个原因，我们将处理（已标注）句子层次而不是词汇层次的数据。
    我们以加载将要使用的数据开始。
        >>> from nltk.corpus import brown
        >>> brown_tagged_sents = brown.tagged_sents(categories='news')
        >>> brown_sents = brown.sents(categories='news')

5.4.1 默认标注器
    最简单的标注器是为每个标识符分配同样的标记。

    为了得到最好的效果，我们用最可能的标记标注每个词。
    让我们找出哪个标记是最优可能的。

    默认的标注器给每一个单独的词分配标记，即使是之前从未遇到过的词。
    碰巧的是，一旦我们处理了几千词的英文文本后，大多数新词都是名词。

    so，默认标注器可以帮我们提高语言处理系统的稳定性。

        >>> tags =[tag for (word,tag) in brown.tagged_words(categories='news')]
        >>> nltk.FreqDist(tags).max()
        'NN'
        >>> raw = 'I do not like green eggs and larm, I do not like them Sam I am !'
        >>> tokens = nltk.word_tokenize(raw)
        >>> default_tagger = nltk.DefaultTagger('NN')
        >>> default_tagger.tag(tokens)
        [('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'), ('eggs', 'NN'), ('and', 'NN'), ('larm', 'NN'), (',', 'NN'), ('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'), ('I', 'NN'), ('am', 'NN'), ('!', 'NN')]
        >>> default_tagger.evaluate(brown_tagged_sents)
        0.13089484257215028



5.4.2 正则表达式标注器
    正则表达式标注器基于匹配模式分配标记给标识符。
    例如：我们可能会猜测任一以ed结尾的词都是动词过去分词，任一以's 结尾的词都是名词所有格。
    可以用一个正则表达式的列表表示这些：

        >>> patterns = [
            ... (r'.*ing$', 'VBG'), # gerunds
            ... (r'.*ed$', 'VBD'), # simple past
            ... (r'.*es$', 'VBZ'), # 3rd singular present
            ... (r'.*ould$', 'MD'), # modals
            ... (r'.*\'s$', 'NN$'), # possessive nouns
            ... (r'.*s$', 'NNS'), # plural nouns
            ... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers
            ... (r'.*', 'NN') # nouns (default)
            ... ]
    请注意，这些是顺序处理的，第一个匹配上的会被使用。

    现在我们可以建立一个标注器，并用它来标记一个句子。
        >>> regexp_tagger = nltk.RegexpTagger(patterns)
        >>> regexp_tagger.tag(brown_sents[3])
        [('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'),
        ('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'),
        ("''", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'),
        ('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ...]
        >>> regexp_tagger.evaluate(brown_tagged_sents)
        0.20326391789486245
    最终的正则表达式«.*»是一个全面捕捉的，标注所有词为名词。
    除了作为正则表达式标注器的一部分重新指定这个，这与默认标注器是等效的（只是效率低得多）。
    有没有办法结合这个标注器和默认标注器呢？我们将很快看到如何做到这一点。


5.4.3 查询标注器
    很多高频词没有NN 标记。让我们找出100 个最频繁的词，存储它们最有可能的标记。
    然后我们可以使用这个信息作为“查找标注器”（NLTK UnigramTagger）的模型：
        >>> fd = nltk.FreqDist(brown.words(categories='news'))
        >>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))
        >>> most_freq_words = []
        >>> for e in fd.keys():
        ...     if len(most_freq_words)<=100:
        ...             most_freq_words.append(e)
        ...     else:
        ...             break
        ...
        >>> most_freq_words
        ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', "Atlanta's", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', "''", 'that', 'any', 'irregularities', 'took', 'place', '.', 'jury', 'further', 'in', 'term-end', 'presentments', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'deserves', 'praise', 'and', 'thanks', 'Atlanta', 'for', 'manner', 'was', 'conducted', 'September-October', 'term', 'been', 'charged', 'by', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'possible', 'hard-fought', 'won', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', 'Only', 'a', 'relative', 'handful', 'such', 'received', 'considering', 'widespread', 'interest', 'number', 'voters', 'size', 'this', 'city', 'it', 'did', 'find', 'many', "Georgia's", 'registration', 'laws', 'are', 'outmoded', 'or', 'inadequate', 'often', 'ambiguous', 'It', 'recommended', 'legislators', 'act', 'have', 'these']
        >>> likely_tags = dict((word,cfd[word].max()) for word in most_freq_words)
        >>> likely_tags
        {'The': 'AT', 'Fulton': 'NP-TL', 'County': 'NN-TL', 'Grand': 'JJ-TL', 'Jury': 'NN-TL', 'said': 'VBD', 'Friday': 'NR', 'an': 'AT', 'investigation': 'NN', 'of': 'IN', "Atlanta's": 'NP$', 'recent': 'JJ', 'primary': 'NN', 'election': 'NN', 'produced': 'VBD', '``': '``', 'no': 'AT', 'evidence': 'NN', "''": "''", 'that': 'CS', 'any': 'DTI', 'irregularities': 'NNS', 'took': 'VBD', 'place': 'NN', '.': '.', 'jury': 'NN', 'further': 'JJR', 'in': 'IN', 'term-end': 'NN', 'presentments': 'NNS', 'the': 'AT', 'City': 'NN-TL', 'Executive': 'NN-TL', 'Committee': 'NN-TL', ',': ',', 'which': 'WDT', 'had': 'HVD', 'over-all': 'JJ', 'charge': 'NN', 'deserves': 'VBZ', 'praise': 'NN', 'and': 'CC', 'thanks': 'NNS', 'Atlanta': 'NP', 'for': 'IN', 'manner': 'NN', 'was': 'BEDZ', 'conducted': 'VBN', 'September-October': 'NP', 'term': 'NN', 'been': 'BEN', 'charged': 'VBN', 'by': 'IN', 'Superior': 'JJ-TL', 'Court': 'NN-TL', 'Judge': 'NN-TL', 'Durwood': 'NP', 'Pye': 'NP', 'to': 'TO', 'investigate': 'VB', 'reports': 'NNS', 'possible': 'JJ', 'hard-fought': 'JJ', 'won': 'VBD', 'Mayor-nominate': 'NN-TL', 'Ivan': 'NP', 'Allen': 'NP', 'Jr.': 'NP', 'Only': 'RB', 'a': 'AT', 'relative': 'JJ', 'handful': 'NN', 'such': 'JJ', 'received': 'VBD', 'considering': 'IN', 'widespread': 'JJ', 'interest': 'NN', 'number': 'NN', 'voters': 'NNS', 'size': 'NN', 'this': 'DT', 'city': 'NN', 'it': 'PPS', 'did': 'DOD', 'find': 'VB', 'many': 'AP', "Georgia's": 'NP$', 'registration': 'NN', 'laws': 'NNS', 'are': 'BER', 'outmoded': 'JJ', 'or': 'CC', 'inadequate': 'JJ', 'often': 'RB', 'ambiguous': 'JJ', 'It': 'PPS', 'recommended': 'VBD', 'legislators': 'NNS', 'act': 'NN', 'have': 'HV', 'these': 'DTS'}
        >>> baseline_tagger = nltk.UnigramTagger(model=likely_tags)
        >>> baseline_tagger.evaluate(brown_tagged_sents)
        0.33352228653260935

    仅仅知道100个最频繁的词的标记就使我们能够正确标注很大一部分标识符。

    让我们来看看它在未标注的输入文本上做的如何：
        >>> sent = brown.sents(categories='news')[3]
        >>> sent
        ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', "''", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', "''", '.']
        >>> baseline_tagger.tag(sent)
        [('``', '``'), ('Only', 'RB'), ('a', 'AT'), ('relative', 'JJ'), ('handful', 'NN'), ('of', 'IN'), ('such', 'JJ'), ('reports', 'NNS'), ('was', 'BEDZ'), ('received', 'VBD'), ("''", "''"), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('considering', 'IN'), ('the', 'AT'), ('widespread', 'JJ'), ('interest', 'NN'), ('in', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('the', 'AT'), ('number', 'NN'), ('of', 'IN'), ('voters', 'NNS'), ('and', 'CC'), ('the', 'AT'), ('size', 'NN'), ('of', 'IN'), ('this', 'DT'), ('city', 'NN'), ("''", "''"), ('.', '.')]

    许多词都被分配了一个None的标签，因为它们不在100个最频繁的词中。
    在这些情况下，我们想分配默认标记为NN。

    换句话说，我们要先使用查找表，如果它不能指定一个标记就使用默认标注器，这个过程叫做 回退。
    我们可以通过指定一个标注器作为另一个标注器的参数做到这个。

    现在查找标注器将只存储名词以外的词的词-标记对。


5.4.4 评估
5.5 N-gram标注
    一元标注（Unigram Tagging)
    分离训练和测试数据
    一般的N-gram的标注
    组合标注器
    标注生词
    存储标注器
    性能限制
    跨句子边界标注
5.6 基于转换的标注
5.7 如何确定一个词的分类
    形态学线索
    句法线索
    语义线索
    新词
    词性标注集中的形态学
5.8 小结