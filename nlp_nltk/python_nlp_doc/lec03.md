第3章 加工原料文本
3.1 从网络和硬盘访问文本
    电子书
    处理的HTML
    处理搜索引擎的结果
    读取本地文件
    从PDF、MS Word及其他二进制格式中提取文本
    捕获用户输入
    NLP的流程
3.2 字符串：最底层的文本处理
3.3 使用Unicode进行文字处理
3.4 使用正则表达式检测词组搭配
3.5 正则表达式的有益应用
    提取字符块
    在字符块上做更多事情
    查找词干
    搜索已分词文本
3.6 规范化文本
    词干提取器
    词形归并
3.7 用正则表达式为文本分词
    分词的简单方法
    NLTK的正则表达式分词器
    分词的进一步问题
3.8 分割
    断句
    分词
3.9 格式化：从链表到字符串

    文本的最重要来源无疑是网络。


    本章的目的是要回答下列问题：
        1. 我们怎样才能编写程序访问本地和网络上的文件，从而获得无限的语言材料？
        2. 我们如何把文档分割成单独的词和标点符号，这样我们就可以开始像前面章节中在
        文本语料上做的那样的分析？
        3. 我们怎样编程程序产生格式化的输出，并把结果保存在一个文件中？
    为了解决这些问题，我们将讲述NLP 中的关键概念，包括分词和词干提取。
    在此过程中，你会巩固你的Python 知识并且了解关于字符串、文件和正则表达式知识。
    既然这些网络上的文本都是HTML 格式的，我们也将看到如何去除HTML 标记。

3.1 从网络和硬盘访问文本
3.1.1 电子书
    requests, urlopen等获取，本地open读取等。
        >>> import nltk
        >>> s = 'I am chinese.'
        >>> tokens = nltk.word_tokenize(s)
        >>> tokens
        ['I', 'am', 'chinese', '.']
        >>> text = nltk.Text(tokens)
        >>> text
        <Text: I am chinese ....>
        >>> type(text)
        <class 'nltk.text.Text'>
        >>> text.collocations()
        # 无返回值



3.1.2 处理的HTML

3.1.3 处理搜索引擎的结果
    网络可以被看作未经标注的巨大的语料库。
    网络搜索引擎提供了一个有效的手段，搜索大量文本作为有关的语言学的例子。
    搜索引擎的主要优势是规模：因为你正在寻找这样庞大的一个文件集，会更容易找到你感兴趣语言模式。
                          而且，你可以使用非常具体的模式，仅仅在较小的范围匹配一两个例子，但在网络上可能匹配成千上万的例子。
    网络搜索引擎的第二个优势是非常容易使用。因此，它是一个非常方便的工具，可以快速检查一个理论是否合理。

    不幸的是，搜索引擎有一些显著的缺点。首先，允许的搜索方式的范围受到严格限制。
    不同于本地驱动器中的语料库，你可以编写程序来搜索任意复杂的模式，搜索引擎一般只允许你搜索单个词或词串，有时也允许使用通配符。
    其次，搜索引擎给出的结果不一致，并且在不同的时间或在不同的地理区域会给出非常不同的结果。
    当内容在多个站点重复时，搜索结果会增加。
    最后，搜索引擎返回的结果中的标记可能会不可预料的改变，基于模式的方法定位特定的内容将无法使用（通过使用搜索引擎APIs 可以改善这个问题）。

3.1.4 读取本地文件
    open
3.1.5 从PDF、MS Word及其他二进制格式中提取文本
    专门的读取包。
3.1.6 捕获用户输入
    input（'please input some text:')
3.1.7 NLP的流程
     获取文本，格式化处理，分词.


3.2 字符串：最底层的文本处理

3.3 使用Unicode进行文字处理
    编码，解码

3.4 使用正则表达式检测词组搭配

3.5 正则表达式的有益应用
    提取字符块
    在字符块上做更多事情
    查找词干
    搜索已分词文本
3.6 规范化文本
3.6.1 词干提取器
    NLTK中包括了一些现成的词干提取器，如果你需要一个词干提取器，你应该优先使用它们中的一个，而不是使用正则表达式制作自己的词干提取器。
    因为NLTK中的词干提取器能处理的不规则的情况很广泛。

    Porter和Lancaster词干提取器按照它们自己的规则剥离词缀。

    例如：
        Porter词干提取器正确处理了词lying(将它映射为lie)，而Lancaster词干提取器并没有处理好。

        >>> porter = nltk.PorterStemmer()
        >>> lancaster = nltk.LancasterStemmer()
        >>> porter.stem('lying')
        'lie'
        >>> lancaster.stem('lying')
        'lying'

    词干提取过程没有明确定义，我们通常选择心目中最适合我们的应用的词干提取器。
    如果你要索引一些文本和使搜索支持不同词汇形式的话，Porter词干提取器是一个很好的选择。

3.6.2 词形归并
    WordNet词形归并器 删除词缀产生的词都是在它的字典中的词。
    这个额外的检查过程使词形归并器比刚才提到的词干提取器要慢。
        >>> wnl = nltk.WordNetLemmatizer()
        >>> wnl.lemmatize('lying')  # 它并未对lying进行处理
        'lying'
        >>> wnl.lemmatize('women')  # 而是将women转换为woman
        'woman'

    如果你想编译一些文本的词汇，或者想要一个有效词条（或中心词）列表，WordNet词形归并器是一个不错的选择。

    另一个规范化任务设计识别 非标准词，包括数字、缩写、日期以及任何此类标识符到一个特殊的词汇的映射。

    例如：
        每一个十进制数可以被映射到一个单独的标识符0.0，每首字母缩写可以映射为AAA。
        这使词汇量变量，提高了许多语言建模任务的准确性。

3.7 用正则表达式为文本分词
    分词是将字符串切割成可识别的构成一块语言数据的语言单元。



3.7.1 分词的简单方法
    str.split(' ')
    re.split(r' ',raw)
    re.split(r'[ \t\n]', raw)
    re.split(r'\W+',raw)

3.7.2 NLTK的正则表达式分词器

    set(tokens).difference(wordlist) 通过比较分词结果与一个词表，然后报告任何没有在词表出现的标识符，来评估一个分词器。

3.7.3 分词的进一步问题
    没有单一的解决方案能在所有领域都行之有效，我们必须根据应用领域的需要决定那些是标识符。
    在开发分词器时，访问已经手工标注好的原始文本是有益的，这可以让你的分词器的输出结果与高品质（或称“黄金标准”）的标注进行比较。
    NLTK 语料库集合包括宾州树库的数据样本，包括《华尔街日报》原始文本（nltk.corpus.treebank_raw.raw()）和分好词的版本（nltk.corpus.treebank.words()）。

3.8 分割
3.8.1 断句
    在词级水平处理文本通常假定能够将文本划分成单个句子。
    正如我们已经看到，一些语料库已经提供在句子级别的访问。
        >>> from nltk.corpus import gutenberg
        >>> len(gutenberg.words())/len(gutenberg.sents())
        26.601317071190845

    在其他情况下，文本可能只是作为一个字符流。
    在将文本分词之前，我们需要将它分割成句子。
    NLTK通过包含Punkt句子分割器简化了这些。

    例如：小说文本断句的例子。
        >>> sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        >>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
        >>> sents = sent_tokenizer.tokenize(text)
        >>> sents[171:173]
        ['In the wild events which were to follow this girl had no\npart at all; he never saw her again until all his tale was over.', 'And yet, in some indescribable way, she kept recurring like a\nmotive in music through all his mad adventures afterwards, and the\nglory of her strange hair ran like a red thread through those dark\nand ill-drawn tapestries of the night.']

    断句是困难的，因为句号会被用来标记缩写而另一些句子同时标记缩写和句子结束，就像发生在缩写如"U.S.A"上的那样。



3.8.2 分词
    由于没有词边界的可视表示，文本分词将变得更加困难--如：中文
    "爱国人" --爱/国人 --/爱国/人


    现在分词的任务变成了一个搜索问题：找到将文本字符串正确分割成词汇的字位串。
    我们假定学习者接收词，并将它们存储在一个内部词典中。
    给定一个合适的词典，是能够由词典中的词的序列来重构源文本的。
    读过(Brent & Cart-wright, 1995)之后，我们可以定义 一个目标函数，一个打分函数，我们将基于词典的大小和从词典中重构源文本所需的信息量尽
力优化它的值。

    计算目标函数：
        给定一个假设的源文本的分词（左），推导出一个词典和推导表，它能让源文本重构，然后合计每个词项（包括边界标志）与推导表的字符数，
        作为分词质量的得分；得分值越小表明分词越好。


    例3-4. 使用模拟退火算法的非确定性搜索：一开始仅搜索短语分词；
        随机扰动0 和1，它们与“温度”成比例；每次迭代温度都会降低，扰动边界会减少。

    有了足够的数据，就可能以一个合理的准确度自动将文本分割成词汇。
    这种方法可用于为那些词的边界没有任何视觉表示的书写系统分词。


3.9 格式化：从链表到字符串
3.10 小结
1. 分词是将文本分割成基本单位或标记，例如词和标点符号等。
   基于空格符的分词对于许多应用程序都是不够的，因为它会捆绑标点符号和词。
   NLTK 提供了一个现成的分词器nltk.word_tokenize()。
2. 词形归并是一个过程，将一个词的各种形式（如：appeared，appears）映射到这个词标准的或引用的形式，也称为词位或词元（如：appear）。