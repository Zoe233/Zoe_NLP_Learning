谱聚类算法原理介绍
前言：学习谱聚类，最好有一些图论、矩阵分解（SVD）方面的知识，
这样会更加有利于谱聚类的学习。

当然，谱聚类理解起来并不困难，实际操作也大多是谱聚类+K-means聚类联合使用的。

一、什么是谱聚类
谱聚类算法是建立在 图论–图谱理论 基础上的，
它的本质是将聚类问题转化为图的最优划分问题。

谱 —— 可以理解为矩阵的特征值的集合；
之前的聚类算法例如：
    K-means、
    DBSCAN、
    局部聚类算法都是点对点的聚类方式，
而谱聚类是把点对点转化成图的相关知识，用图论里的最优划分来解决点的聚类问题。


二、图的简单介绍
谱聚类方式就是要把点与点转化成图，
那么图论的相关知识就简单介绍一下，
如果想详细了解，推荐学习一下《图论》或者《离散数学》里面都有更为具体的介绍。

2.1、图的表示
构成图的三要素——
    节点，
    连接节点的边，
    以及边上的权重；

如果给图的每条边规定一个方向，那么得到的图称为有向图。
在有向图中，与一个节点相关联的边有出边和入边之分。

相反，边没有方向的图称为无向图。
一般用G(V,E)表示无向图，V={v1,v2,…,vn}表示点集，E表示边集。
每条边上有wij表示vi与vj之间关系，被称为权重，

对应无向图是有wij=wji，且wii=0，wij>=0。
例如下图展示的是一个无向图，边上的数值表示权重


2.2、图的划分
图的划分：
    是将图完全划分为若干个子图，各子图间无交集。
    也就是说将原图转化成一个完备数据组。
划分要求：
    同子图内的点相似度高，不同子图内的点相似度低 —— 像K-means的类内紧凑，类间独立。

划分的损失函数：划分时子图之间被“截断”的边的权重和，例如图下，划分两个区域的最小损失函数=0.1+0.2


那么：
问题1来了：
    上面是一个“恰好完美”分成两份的例图。我们该如何通过数学推导来证明上图的分割成两份是最完美的呢？
问题2来了：
    那我想把上图“完美”分成三份、分成四份，我又该怎么切分呢？

3.1、图的邻接矩阵W
图的邻接矩阵是和图的每条边的方向、每条边的权重有关系的。具体且简单的说：
在无向图中，如下：
①号点与②号点权重是0.8，所以1–>2和2–>1的值都是0.8；
①号点与④号点没有直接连接，所以相互权重都是0；自身到自身也是0
这样就可以得到图的邻接矩阵了。

而有向图的邻接矩阵，要根据各点与各点边的方向，以及权重确定的。
所以无向图的邻接矩阵一定是一个对称矩阵，有向图则不一定。

3.2、图的度矩阵D
图的度，在无向图中即为连接某一点所有边的权重的总和。
而在有向图中由于边的方向导致某一点连接其他的点会有出度、入度之分——
指向该点为入度，该点发出为出度，此时有向图的度=出度-入度。

例如上图是无向图，所以①点的度=0.8+0.6+0.1=1.5

3.3、图的拉普拉斯矩阵L
图的拉普拉斯矩阵L是与图的邻接矩阵W、度矩阵D有关系的，关系式为：L = D - W

最后对图的拉普拉斯矩阵L做SVD分解——
    也就是求出矩阵L的特征值所对应的特征向量，通过特征向量来对图进行划分，上图例子的结果如下：

说明几点：

为什么第一列特征值是相同的值：因为这是无向图，无向图的拉普拉斯矩阵每一列或每一行加起来都是等于1的，也就是说矩阵肯定有一个特征值=0，那么0所对应的特征向量=1（等于1就是相同的意思）
怎么根据特征向量做图形划分：划分N类，就取前N个特征向量组成的点集，再用K-means聚成N类进行划分。
比如划分两类，就取前两列特征向量。而由于这是无向图，所以第一列要不要无所谓（无向图的第一列相当于降维了），只根据第二列进行K-means聚成两类，很明显①②③是同类，④⑤⑥是又一类。
比如划分三类，就取前三列特征向量，无向图就相当于组成了一个二维坐标系中的点集——第一个点坐标是(-0.408,-0.647)，第二个点坐标是(-0.442,0.014)，这样在二维坐标系中用K-means聚成三类即完成。
特别说明有向图第一列就不能省略了哦

3.4、谱聚类算法分析
输入：样本及类别数K
    1.根据样本建立权重矩阵W
    2.根据W，计算度矩阵D，进而计算拉普拉斯矩阵L
    3.计算L的特征值及特征向量V_{e} = {v_{e1},v_{e2},...v_{en}};
    4.取出前k小特征值对应的特征向量V_{K} = {v_{e1},v_{e2},...v_{ek}}并对矩阵V_{K}的行向量进行聚类，得到K个Cluster

