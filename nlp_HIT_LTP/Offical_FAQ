1.LTP是什么？
    "语言云"以哈工大社会计算与信息检索研究中心研发的"语言技术平台"LTP为基础，
    为用户提供高效精准的中文自然语言处理云服务。

2.LTP如何使用？
    只需要根据API参数构造HTTP请求即可在线获得分析结果，无需下载SDK，无需购买高性能的机器，
    同时支持跨平台、跨语言编程等。

3.LTP的架构：
    基于 XML 的语言处理结果表示，
    并在此基础上提供了一整套自底向上的丰富、高效、高精度的中文自然语言处理模块。

    词法、句法、语义 等5项中文处理核心技术。


 倒数2层：   词法分析                        句法分析         语义分析
           分词，词性标注，命名实体识别       依存句法分析     语义角色标注

 底层：      基于XML的语言学知识资源和语料库资源

4. 分词
    中文分词 Word Segmentation,WS
    指的是将汉字序列切分成词序列。

    在汉语中，词是承载语义的最基本的单元。
    分词是信息检索、文本分类、情感分析等多项中文自然语言处理任务的基础。

    难点：
        切分歧义
    解决：
        机器学习
        自定义词典，便捷地加入新词。

5. 词性标注
    词性标注Part-of-speech Tagging,POS
    是给句子中的每个词一个词性类别的任务。

    这里的词性类别可能是 名词、动词、形容词和其他。

    词性作为对词的一种泛化，在语言识别、句法分析、信息抽取等任务中有重要作用。

6. 命名实体识别
    命名实体识别 Named Entity Recognition, NER
    是在句子的词序列中定位并识别人名、地名、机构名等实体的任务。

    例子：
        国务院 (机构名) 总理李克强 (人名) 调研上海外高桥 (地名) 时提出，支持上海 (地名) 积极探索新机制。

    命名实体识别对于挖掘文本中的实体进而对其进行分析有很重要的作用。

    命名实体识别的类型一般是根据任务确定的。
    LTP提供最基本的三种实体类型 人名、地名、机构名 的识别。

    用户可以很容易将实体类型拓展，比如：品牌名、软件名等实体类型。

7. 依存句法分析
    依存句法分析 Dependency Parsing, DP
    通过分析语言单位内成分之间的依存关系揭示其句法结构。

    直观来讲，依存句法分析识别句子中的"主谓宾"，"定状补"这些语法成分，并分析各成分之间的关系。

    依存句法分析标注关系 (共15种) 及含义如下：

        关系类型	Tag	Description	Example
        主谓关系	SBV	subject-verb	我送她一束花 (我 <-- 送)
        动宾关系	VOB	直接宾语，verb-object	我送她一束花 (送 --> 花)
        间宾关系	IOB	间接宾语，indirect-object	我送她一束花 (送 --> 她)
        前置宾语	FOB	前置宾语，fronting-object	他什么书都读 (书 <-- 读)
        兼语	DBL	double	他请我吃饭 (请 --> 我)
        定中关系	ATT	attribute	红苹果 (红 <-- 苹果)
        状中结构	ADV	adverbial	非常美丽 (非常 <-- 美丽)
        动补结构	CMP	complement	做完了作业 (做 --> 完)
        并列关系	COO	coordinate	大山和大海 (大山 --> 大海)
        介宾关系	POB	preposition-object	在贸易区内 (在 --> 内)
        左附加关系	LAD	left adjunct	大山和大海 (和 <-- 大海)
        右附加关系	RAD	right adjunct	孩子们 (孩子 --> 们)
        独立结构	IS	independent structure	两个单句在结构上彼此独立
        标点	WP	punctuation	。
        核心关系	HED	head	指整个句子的核心

8. 语义角色标注
    语义角色标注 Semantic Role Labeling, SRL
    是一种浅层的语义分析技术，标注句子中某些短语为给定谓语的论元（语义角色），
    如 施事、受事、时间和地点等。

    其能够对问答系统、信息抽取和机器翻译等应用产生推动作用。

    核心的语义角色为 A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。其余的15个语义角色为附加语义角色，如LOC 表示地点，TMP 表示时间等。附加语义角色列表如下：

        标记	说明
        ADV	adverbial, default tag ( 附加的，默认标记 )
        BNE	beneﬁciary ( 受益人 )
        CND	condition ( 条件 )
        DIR	direction ( 方向 )
        DGR	degree ( 程度 )
        EXT	extent ( 扩展 )
        FRQ	frequency ( 频率 )
        LOC	locative ( 地点 )
        MNR	manner ( 方式 )
        PRP	purpose or reason ( 目的或原因 )
        TMP	temporal ( 时间 )
        TPC	topic ( 主题 )
        CRD	coordinated arguments ( 并列参数 )
        PRD	predicate ( 谓语动词 )
        PSR	possessor ( 持有者 )
        PSE	possessee ( 被持有 )


9. 语义依存分析
    语义依存分析 Semantic Dependency Parsing, SDP
    分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。

    使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，
    而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多的。

    语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。

    语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。
    这也是语义依存分析与句法依存分析的重要区别。


10. 各模块技术指标
    10.1 分词
        主流的分词算法包括 基于词典匹配的方法 和 基于统计机器学习的方法。

        LTP分词模块使用的算法将两种方法进行了融合，算法既能利用机器学习较好的消歧能力，又能灵活地引入词典等外部资源。

        在LTP中，我们将分词任务建模为 基于字的序列标注问题 。
        对于输入句子的字序列，模型给句子中的每个字标注一个标识词边界的标记。
        同时，为了提高互联网文本特别是微博文本的处理性能。我们在分词系统中加入如下一些优化策略：
            英文、URI一类特殊词识别规则
            利用空格等自然标注线索
            在统计模型中融入词典信息
            从大规模未标注数据中统计字间互信息、上下文丰富程度
        性能指标：
            Precision, Recall, F1

        推论：应该是使用的CRF的方式对 基于字的序列进行了S,BE,BME的标注。

    10.2 词性标注
        将词性标注任务建模为基于词的序列标注问题。

        采用北大标注集。


    10.3 命名实体识别
        将命名实体识别建模为基于词的序列标注问题。对于输入句子的词序列，
        模型给句子中的每个词标注一个标识命名实体边界和实体类别的标记。
        在LTP中，我们支持人名、地名、机构名三类命名实体的识别。
    10.4 依存句法分析
        基于图的依存分析方法由McDonald首先提出，
        他将依存分析问题归结为在一个有向图中寻找最大生成树（Maximum Spanning Tree）的问题。

        在依存句法分析模块中，LTP分别实现了
            一阶解码(1o)
            二阶利用子孙信息解码(2o-sib)
            二阶利用子孙和父子信息(2o-carreras)

    10.5 语义角色标注
        们将SRL分为两个子任务，
            其一是谓词的识别（Predicate Identification, PI），
            其次是论元的识别以及分类（Argument Identification and Classification, AIC）。
        对于论元的识别及分类，我们将其视作一个联合任务，即将“非论元”也看成是论元分类问题中的一个类别。
        在SRL系统中，我们在最大熵模型中引入L1正则，使得特征维度降至约为原来的1/40，从而大幅度地减小了模型的内存使用率，并且提升了预测的速度。
        同时，为了保证标注结果满足一定的约束条件，系统增加了一个后处理过程。
