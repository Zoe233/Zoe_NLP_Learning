{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foolNLTK包的解析过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一步：查看各个路径，其中/test/dictonary.py文件可以直接用于跑和测试\n",
    "# so,直接从此处作为入手开始解析代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fool\n",
    "text = [\"我在北京天安门看你难受香菇,一一千四百二十九\", \"我在北京晒太阳你在非洲看雪\", \"千年不变的是什么\", \"我在北京天安门。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('no dict:',\n",
       " [['我', '在', '北京', '天安门', '看', '你', '难受', '香菇', ',', '一一', '千四百', '二十九'],\n",
       "  ['我', '在', '北京', '晒太阳', '你', '在', '非洲', '看', '雪'],\n",
       "  ['千', '年', '不', '变', '的', '是', '什么'],\n",
       "  ['我', '在', '北京', '天安门', '。']])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'no dict:',fool.cut(text,ignore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('use dict:',\n",
       " [['我', '在', '北京天安门', '看', '你', '难受香菇', ',', '一', '一千', '四百', '二十', '九'],\n",
       "  ['我', '在', '北京', '晒太阳', '你', '在', '非洲', '看', '雪'],\n",
       "  ['千', '年', '不', '变', '的', '是', '什么'],\n",
       "  ['我', '在', '北京天安门', '。']])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载用户自定义词典\n",
    "user_dict_path = os.path.join(os.getcwd(),'FoolNLTK-master/test/test_dict.txt')\n",
    "fool.load_userdict(user_dict_path)\n",
    "'use dict:',fool.cut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('deleted dict:',\n",
       " [['我', '在', '北京', '天安门', '看', '你', '难受', '香菇', ',', '一一', '千四百', '二十九'],\n",
       "  ['我', '在', '北京', '晒太阳', '你', '在', '非洲', '看', '雪'],\n",
       "  ['千', '年', '不', '变', '的', '是', '什么'],\n",
       "  ['我', '在', '北京', '天安门', '。']])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除用户自定义词典\n",
    "fool.delete_userdict()\n",
    "'deleted dict:',fool.cut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解析：\n",
    "# 从上面的fool.load_userdict()方法的调用，转向/fool目录下的__init__.py文件，开始查看其中的内容。\n",
    "# 暂时下面的fool.pos_cut()词性标注，和fool.analysis()，fool.ner()命名实体识别方法都是同样的查看方法。\n",
    "# 故，一起转向/foo/__init__.py文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos result [[('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('天安门', 'ns'), ('看', 'v'), ('你', 'r'), ('难受', 'a'), ('香菇', 'n'), (',', 'wd'), ('一一', 'd'), ('千四百', 'v'), ('二十九', 'm')], [('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('晒太阳', 'nz'), ('你', 'r'), ('在', 'p'), ('非洲', 'ns'), ('看', 'v'), ('雪', 'n')], [('千', 'm'), ('年', 'q'), ('不', 'd'), ('变', 'v'), ('的', 'ude'), ('是', 'vshi'), ('什么', 'r')], [('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('天安门', 'ns'), ('。', 'wj')]]\n"
     ]
    }
   ],
   "source": [
    "pos_words =fool.pos_cut(text)\n",
    "print(\"pos result\", pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: [[('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('天安门', 'ns'), ('看', 'v'), ('你', 'r'), ('难受', 'a'), ('香菇', 'n'), (',', 'wd'), ('一一', 'd'), ('千四百', 'v'), ('二十九', 'm')], [('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('晒太阳', 'nz'), ('你', 'r'), ('在', 'p'), ('非洲', 'ns'), ('看', 'v'), ('雪', 'n')], [('千', 'm'), ('年', 'q'), ('不', 'd'), ('变', 'v'), ('的', 'ude'), ('是', 'vshi'), ('什么', 'r')], [('我', 'r'), ('在', 'p'), ('北京', 'ns'), ('天安门', 'ns'), ('。', 'wj')]]\n",
      "ners:  [[(2, 8, 'location', '北京天安门')], [(2, 5, 'location', '北京'), (9, 12, 'location', '非洲')], [], [(2, 8, 'location', '北京天安门')]]\n",
      "ners: [[(2, 8, 'location', '北京天安门')], [(2, 5, 'location', '北京'), (9, 12, 'location', '非洲')], [], [(2, 8, 'location', '北京天安门')]]\n"
     ]
    }
   ],
   "source": [
    "words, ners = fool.analysis(text)\n",
    "print('words:',words) # 词性标注的结果\n",
    "print(\"ners: \", ners)  # 命名实体识别\n",
    "\n",
    "ners = fool.ner(text)\n",
    "print(\"ners:\", ners)  # 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =========转向目录/fool/__init__.py 解析主程序 =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =========转向目录/fool/__init__.py 解析主程序 =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "# defaultdict 主要用来需要对 value 做初始化的情形\n",
    "# 使用dict时，如果引用的Key不存在，就会抛出KeyError。\n",
    "# 如果希望key不存在时，返回一个默认值，就可以用defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从主目录结构下导入各个功能的类\n",
    "from fool import lexical  # 词汇的\n",
    "from fool import dictionary\n",
    "from fool import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成实例\n",
    "LEXICAL_ANALYSER = lexical.LexicalAnalyzer()\n",
    "_DICTIONARY = dictionary.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging配置\n",
    "__log_console = logging.StreamHandler(sys.stderr)\n",
    "DEFAULT_LOGGER = logging.getLogger(__name__)\n",
    "DEFAULT_LOGGER.setLevel(logging.DEBUG)\n",
    "DEFAULT_LOGGER.addHandler(__log_console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__all__ =['load_model','cut','pos_cut','ner','analysis','load_userdict','delete_userdict']\n",
    "# __all__指定的是指此包被import * 的时候, 哪些模块会被import进来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载模型，具体的模型的搭建和加载等细节等此处完善后转向/fool/model.py文件去解析其内容。\n",
    "def load_model(map_file,model_file):\n",
    "    m = model.Model(map_file = map_file, model_file = model_file)\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _check_input(text, ignore = False):\n",
    "    '''\n",
    "    入参：text,列表格式，\n",
    "        ignore,bool,用于确认是否进行text列表内元素的空值检测。\n",
    "    '''\n",
    "    if not text: # 空值检测\n",
    "        return []\n",
    "    if not isinstance(text,list): # text格式检测，必须为列表，将text变量值转换为列表\n",
    "        text = [text]\n",
    "    null_index = [i for i,t in enumerate(text) if not t]  # 检测列表中每个元素是否存在空值，如果存在则记录下其索引值。\n",
    "    if null_index and not ignore:\n",
    "        raise Exception('null text in input')\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text, ignore = False):\n",
    "    text = _check_input(text,ignore)  # 空值检测\n",
    "    if not text:\n",
    "        return [[]]\n",
    "    res = LEXCICAL_ANALYSER.ner(text)   # 调用LEXCICAL_ANAYSER中的ner方法，和model一样，后续分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis(text, ignore = False):\n",
    "    text = _check_input(text,ignore)\n",
    "    if not text:\n",
    "        return [[]],[[]]\n",
    "    res = LEXICAL_ANALYSER.analysis(text) # 同上ner方法\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(text,ignore = False):\n",
    "    text = _check_input(text,ignore)\n",
    "    if not text:\n",
    "        return [[]]\n",
    "    text = [t for t in text if t]\n",
    "    all_words = LEXICAL_ANALYSER.cut(text)  # 同上\n",
    "    new_words = []\n",
    "    if _DICTIONARY.sizes !=0:  # 根据dictionary中的sizes变量\n",
    "        for sent, words in zip(text,all_words):\n",
    "            words = _mearge_user_words(sent, words)  \n",
    "                # 如果执行，调用本脚本文件类中的_mearge_user_words()方法，接下来即将查看\n",
    "            new_words.append(words)\n",
    "    else:\n",
    "        new_words = all_words\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_cut(text):\n",
    "    # 先分词\n",
    "    words = cut(text)\n",
    "    # 再将分词的结果给到词性标注方法pos\n",
    "    pos_labels = LEXICAL_ANALYSER.pos(words)\n",
    "    word_inf = [list(zip(ws,ps)) for ws,ps in zip(words, pos_labels)]\n",
    "    return word_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_userdict(path):\n",
    "    _DICTIONARY.add_dict(path)  # 后续查看dictionary中的add_dict方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_userdict():\n",
    "    _DICTIONARY.delete_dict()  # 同上dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 承接上面cut方法中，如果_DICTIONARY.sizes !=0 时对分词结果做的处理方式\n",
    "def _mearge_user_words(text,seg_results):\n",
    "    '''\n",
    "    是否加载用户自定义词典的方式\n",
    "    dictionary.sizes应该是在load_userdict()方法中会被改变，记录的是自定义词典的数量。\n",
    "    入参：text 是 sents,字符串\n",
    "        seg_results是words列表\n",
    "    '''\n",
    "    if not _DICTIONARY:\n",
    "        return seg_results\n",
    "    matchs = _DICTIONARY.parse_words(text)\n",
    "    graph = defaultdict(dict)  # key不存在的默认值为{},如graph['0']={}\n",
    "    text_len = len(text)\n",
    "    \n",
    "    for i in range(text_len):\n",
    "        graph[i][i+1] = 1.0  # {i:{i+1:1.0}}\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for w in seg_results:\n",
    "        w_len = len(w)\n",
    "        graph[index][index+w_len] = _DICTIONARY.get_weight(w) + w_len\n",
    "        index += w_len\n",
    "    \n",
    "    for m in matchs:\n",
    "        graph[m.start][m.end] = _DICTIONARY.get_weight(m.keyword)*len(m.keyword)\n",
    "    \n",
    "    route ={}\n",
    "    route[text_len] = (0,0)\n",
    "    \n",
    "    for idx in range(text_len -1, -1, -1):\n",
    "        m = [((graph.get(idx).get(k) + route[k][0]),k) for k in graph.get(idx).keys()]\n",
    "        mm = max(m)\n",
    "        route[idx] = (mm[0],mm[1])\n",
    "    \n",
    "    index = 0\n",
    "    path =[index]\n",
    "    words = []\n",
    "    \n",
    "    while index<text_len:\n",
    "        ind_y = route[index][1]\n",
    "        path.append(ind_y)\n",
    "        words.append(text[index:ind_y])\n",
    "        index = ind_y\n",
    "    return words\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面最后一个_mearge_user_words()方法是用来将默认的模型分词方法结果根据\n",
    "# 用户自定义词典的结果比较后，记录索引值，替换等操作。\n",
    "# 具体的细节后面再一步步的解析。\n",
    "# 此处记录下结构信息即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========/fool/__init__.py解析结束============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>> 转向/fool/model.py文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "# 条件随机场的 维特比算法 -- 一种动态规划。\n",
    "# viterbi_decode(score, transition_params)\n",
    "# 作用就是返回最好的标签序列，这个函数只能在测试时使用，在tensorflow外部解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# viterbi_decode(score, transition_params)\n",
    "# 参数：score: 一个形状为[seq_len, num_tags] matrix of unary potentials. \n",
    "#      transition_params: 形状为[num_tags, num_tags] 的转移矩阵\n",
    "# 返回：viterbi: 一个形状为[seq_len] 显示了最高分的标签索引的列表. \n",
    "#      viterbi_score: A float containing the score for the Viterbi sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(logits, trans, sequence_lengths, tag_num):\n",
    "    '''\n",
    "    入参：\n",
    "        logits:\n",
    "        trans:\n",
    "        sequence_lengths:\n",
    "        tag_num:\n",
    "    '''\n",
    "    viterbi_sequences = []\n",
    "    small = -1000.0\n",
    "    start = np.asarray([[small]* tag_num+[0]])\n",
    "    # 此处[[small]*tag_num +[0]] 生成一个[[-1000.0,..., -1000.0, 0]]的列表\n",
    "    # 长度为tag_num+1\n",
    "    # start.shape为(1,tag_num+1)\n",
    "    for logit, length in zip(logits, sequence_lengths):\n",
    "        score = logit[:length]\n",
    "        pad = small * np.ones([length,1])\n",
    "        logits = np.concatenate([score, pad], axis=1)\n",
    "        logits = np.concatenate([start, logits], axis= 0)\n",
    "        viterbi_seq, viterbi_score = viterbi_decode(logits, trans)\n",
    "        viterbi_sequences.append(viterbi_seq[1:])\n",
    "    return viterbi_sequences\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map(path):\n",
    "    '''\n",
    "    加载map\n",
    "    此处为加载/data/map.zip文件\n",
    "    map.zip文件中存储的具体信息可以在lexical.py中查看规则\n",
    "    '''\n",
    "    with open(path, 'rb') as f:\n",
    "        char_to_id ,tag_to_id, id_to_tag = pickle.load(f)\n",
    "    return char_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    '''\n",
    "    读取pd模型，\n",
    "    tf的模型的saver的相关知识需要系统学习和认识一下\n",
    "    '''\n",
    "    with tf.gfile.GFile(path,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name='prefix')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, map_file, model_file):\n",
    "        # load_map是得到字符的id，目标tag的id\n",
    "        # 具体是为了什么，实现什么？\n",
    "        self.char_to_id, self.id_to_tag = load_map(map_file)\n",
    "        # 加载pb文件，加载计算图\n",
    "        self.graph = load_graph(model_file)\n",
    "        \n",
    "        # 加载计算图后，获取tensor\n",
    "        self.input_x = self.graph.get_tensor_by_name('prefix/char_inputs:0')\n",
    "        self.lengths = self.graph.get_tensor_by_name('prefix/lengths:0')\n",
    "        self.dropout = self.graph.get_tensor_by_name('prefix/dropout:0')\n",
    "        self.logits = self.graph.get_tensor_by_name('prefix/project/logits:0')\n",
    "        self.trans = self.graph.get_tensor_by_name('prefix/crf_loss/transitions:0')\n",
    "        \n",
    "        # 生成会话\n",
    "        self.sess = tf.Session(graph = self.graph)\n",
    "        self.sess.as_default()\n",
    "        self.num_class = len(self.id_to_tag)\n",
    "    \n",
    "    def predict(self, sents):\n",
    "        inputs = []\n",
    "        lengths = [len(text) for text in sents]\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        for sent in sents:\n",
    "            send_ids = [self.char_to_id.get(w) if w in self.char_to_id else self.char_to_id.get('<OOV>') for w in sent]\n",
    "            padding = [0] * (max_len - len(sent_ids))\n",
    "            sent_ids += padding\n",
    "            inputs.append(sent_ids)\n",
    "        \n",
    "        inputs = np.array(inputs, dtype=np.int32)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.input_x:inputs,\n",
    "            self.lengths:lengths,\n",
    "            self.dropout:1.0,\n",
    "        }\n",
    "        \n",
    "        # 将向量化好的句子在计算图模型中运行\n",
    "        logits,trans = self.sess.run([self.logits, self.trans], feed_dict = feed_dict)\n",
    "        # 通过维特比算法得到最好的序列\n",
    "        path = decode(logits, trans, lengths, self.num_class) \n",
    "        labels = [[self.id_to_tag.get(l) for l in p] for p in path]\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <<<<<<<< 以上/fool/models.py 看完，但是具体的细节，规范等还需要重新根据tf的各种方式\n",
    "# 重新理解。\n",
    "# 此处的作用：读取/data目录下*.pb文件，加载图，加载map.zip，构建get_tensor_by_name()等，\n",
    "# 将句子文本向量化。作为inputs加载到sess.run(),得到结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>> 接下来转向/fool/lexical.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "from fool.predictor import Predictor\n",
    "from zipfile import ZipFile\n",
    "# zipfile有两个重要的类：ZipFile和ZipInfo\n",
    "# ZipFile类 来创建和读取zip文件而ZipInfo是存储的zip文件的每个文件的信息的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OOV_STR = '<OOV>' # 不晓得干嘛的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _load_map_file(path, char_map_name, id_map_name):\n",
    "    with ZipFile(path) as myzip:\n",
    "        # where is all_map.json ？\n",
    "        # print(myzip.NameToInfo)\n",
    "        # 所以open的name可以使myzip.NameToInfo获取，格式记得处理下\n",
    "        with myzip.open('all_map.json') as myfile:\n",
    "            \n",
    "            content = myfile.readline() # type(content)  为bytes\n",
    "            content = content.decode() # type(content) 为str\n",
    "            # {\"char_map\": {\"<PAD>\": 0, \"<OOV>\": 1, \"</s>\": 2, \"\\uff0c\": ...}\n",
    "            # 是'char_map', 'word_map', 'seg_map', 'pos_map', 'ner_map'\n",
    "            # 五个大key，下面每个key下面对应一堆字典，字典内存储的是unicode字符和数字的对应\n",
    "            \n",
    "            data = json.loads(content) \n",
    "            # 将content的unicode字符串转换为解码为原本指定的编码，如utf-8 或gbk等。\n",
    "            # data的格式：\n",
    "            # {‘char_map':{'<PAD>': 0, '<OOV>': 1, ... '中': 14,...},\n",
    "            #  'word_map':{...'平悦': 141465, 'SC2': 141466, '百色一号': 141467, '局盘龙': 141468, '大步村': 141469,\n",
    "            #  'seg_map':{'0': 'B', '1': 'E', '2': 'S', '3': 'M'},\n",
    "            #  'pos_map':{'0': 'v', '1': 'n', '2': 'a', '3': 'c', '4': 'wt', '5': 'wm', '6': 'nz', '7': 'ns', '8': 'p', '9': 'b', '10': 'ude', '11': 'wd',...},\n",
    "            #  'ner_map':{'0': 'O', '1': 'B_location', '2': 'M_location', '3': 'E_location', '4': 'B_org', ...}}\n",
    "            \n",
    "            return data.get(char_map_name), data.get(id_map_name)\n",
    "                # 两个返回值时char_map和指定的seg_map/pos_map/ner_map的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过解析_load_map_file(path, char_map_name, id_map_name)\n",
    "# 了解其功能和map.zip中的内容\n",
    "# 这个也是train和predict的时候向量化的必要的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalAnalyzer(object):\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.map = None\n",
    "        self.seg_model = None\n",
    "        self.pos_model = None\n",
    "        self.ner_model = None\n",
    "        \n",
    "        # 本地加载map.zip文件的路径配置\n",
    "        self.data_path = os.path.join(sys.prefix,'fool')\n",
    "        # sys.prefix 放置平台独立python文件的路径前缀\n",
    "        # 如本机的路径为/Users/Zoe/anaconda3\n",
    "        self.map_file_path = os.path.join(self.data_path,'map.zip')\n",
    "        \n",
    "    def _load_model(self, model_namel, word_map_name, tag_name):\n",
    "        '''加载map.zip,并指定获取char_map和*_map的字典'''\n",
    "        seg_model_path = os.path.join(self.data_path, model_namel)\n",
    "        char_to_id, id_to_seg = _load_map_file(self.map_file_path,\n",
    "                                              word_map_name,\n",
    "                                              tag_name)\n",
    "        # 此处char_to_id命名ok，id_to_seg的命名不严谨\n",
    "        # 因为_load_model方法是用于加载seg,pos,ner三个不同的id_to_mapname的字典\n",
    "        return Predictor(seg_model_path, char_to_id, id_to_seg)\n",
    "        # 实例化Predictor实例    \n",
    "    \n",
    "    def _load_seg_model(self):\n",
    "        '''\n",
    "        加载seg.pb模型，加载map字典，\n",
    "        生成指定的Predictor实例\n",
    "        '''\n",
    "        self.seg_model = self._load_model('seg.pb','char_map','seg_map')\n",
    "    \n",
    "    def _load_pos_model(self):\n",
    "        self.pos_model = self._load_model('pos.pb','word_map','pos_map')\n",
    "        \n",
    "    def _load_ner_model(self):\n",
    "        self.ner_model = self._load_model('ner.pb','char_map','ner_map')\n",
    "    \n",
    "    # 以上均为 模型加载，map等配置的生成，接下来开始调用功能\n",
    "    \n",
    "    \n",
    "    # 以下分别实现了 词性标注，命名实体识别，分词的功能： pos，ner,cut和analysis\n",
    "    def pos(self, seg_words_list):\n",
    "        '''\n",
    "        调用模型，predict获得词性标注的结果，入参为 分好词的句子链表\n",
    "        根据/fool/__init__.py中发现，词性标注前，会先完成分词，将分词的结果作为入参传入入\n",
    "        '''\n",
    "        if not self.pos_model:\n",
    "            self._load_pos_model()\n",
    "        pos_labels = self.pos_model.predict(seg_words_list)\n",
    "        return pos_labels\n",
    "    \n",
    "    \n",
    "    def ner(self, text_list):\n",
    "        '''\n",
    "        入参：text_list 直接就是['sent1','sent2']这样的句子列表\n",
    "        '''\n",
    "        if not self.ner_model:\n",
    "            self._load_ner_model()\n",
    "        \n",
    "        ner_labels = self.ner_model.predict(text_list) \n",
    "        # ner_labels 的返回结果形如：[['O','O','B_location','M_location','M_location','M_location','E_location','O','O','O','O','O',..],\n",
    "        # ,['O','O','M_location',...],[...],[...]]的双层列表结果。\n",
    "        # 里面一层为每个句子的结果，外面是将所有的句子的结果组合。\n",
    "\n",
    "        all_entitys = []\n",
    "        \n",
    "        for ti,text in enumerate(text_list):\n",
    "            ens = []\n",
    "            entity = ''\n",
    "            i = 0\n",
    "            ner_label = ner_labels[ti]\n",
    "            chars = list(text)\n",
    "            \n",
    "            for label,word in zip(ner_label,chars):\n",
    "                i +=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LexicalAnalyzer类中，根据加载不同的pb文件（模型），map字典，生成不同的Preditor模型\n",
    "# 此处需要细细研究，model部分的代码和preditor部分的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [\"我在北京天安门看你难受香菇,一一千四百二十九\", \"我在北京晒太阳你在非洲看雪\", \"千年不变的是什么\", \"我在北京天安门。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char,ner = _load_map_file('/Users/zoe/PycharmProjects/Zoe_NLP/nlp_foolNLTK/FoolNLTK-master/data/map.zip','char_map','ner_map')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_model = Predictor('/Users/zoe/PycharmProjects/Zoe_NLP/nlp_foolNLTK/FoolNLTK-master/data/ner.pb',char,ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ner_model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B_location',\n",
       " 'M_location',\n",
       " 'M_location',\n",
       " 'M_location',\n",
       " 'E_location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ti,content in enumerate(text):\n",
    "    ens = []\n",
    "    entity = ''\n",
    "    i = 0\n",
    "    ner_label = ner_labels[ti]\n",
    "    chars = list(text)\n",
    "    \n",
    "    for label,word in zip(ner_label,chars):\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity = ''\n",
    "            i = 0\n",
    "            ner_label = ner_labels[ti]\n",
    "            chars = list(text)\n",
    "            \n",
    "            for label,word in zip(ner_label,chars):\n",
    "                i +=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
